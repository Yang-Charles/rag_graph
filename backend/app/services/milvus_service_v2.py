"""
Improved Milvus service (v2) that implements multi-vector hybrid search:
- Semantic text search using dense vectors
- Full-text search using sparse vectors (BM25)
- Image search using dense vectors
Based on Milvus multi-vector hybrid search documentation.
"""
from pymilvus import MilvusClient, DataType, Function, FunctionType, AnnSearchRequest
from typing import List, Tuple, Any
import numpy as np
from sentence_transformers import SentenceTransformer
from PIL import Image
import io
import clip
import torch

class MilvusService:
    def __init__(self, uri="http://127.0.0.1:19530", collection_name="multimodal_docs"):
        self.uri = uri
        self.collection_name = collection_name
        self.client = MilvusClient(uri=uri)
        self.text_model = SentenceTransformer("all-MiniLM-L6-v2")
        # Load CLIP model for image embeddings
        self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device="cpu")

    def create_collection(self, dim_text=384, dim_image=512):
        if self.client.has_collection(self.collection_name):
            return

        schema = self.client.create_schema(auto_id=False)

        # Add fields
        schema.add_field(field_name="doc_id", datatype=DataType.INT64, is_primary=True, description="document id")
        schema.add_field(field_name="text", datatype=DataType.VARCHAR, max_length=65535, enable_analyzer=True, description="raw text")
        schema.add_field(field_name="text_dense", datatype=DataType.FLOAT_VECTOR, dim=dim_text, description="text dense embedding")
        schema.add_field(field_name="text_sparse", datatype=DataType.SPARSE_FLOAT_VECTOR, description="text sparse embedding auto-generated by BM25")
        schema.add_field(field_name="image_dense", datatype=DataType.FLOAT_VECTOR, dim=dim_image, description="image dense embedding")

        # Add BM25 function
        bm25_function = Function(
            name="text_bm25_emb",
            input_field_names=["text"],
            output_field_names=["text_sparse"],
            function_type=FunctionType.BM25,
        )
        schema.add_function(bm25_function)

        # Prepare index parameters
        index_params = self.client.prepare_index_params()
        index_params.add_index(
            field_name="text_dense",
            index_name="text_dense_index",
            index_type="AUTOINDEX",
            metric_type="IP"
        )
        index_params.add_index(
            field_name="text_sparse",
            index_name="text_sparse_index",
            index_type="SPARSE_INVERTED_INDEX",
            metric_type="BM25",
            params={"inverted_index_algo": "DAAT_MAXSCORE"}
        )
        index_params.add_index(
            field_name="image_dense",
            index_name="image_dense_index",
            index_type="AUTOINDEX",
            metric_type="IP"
        )

        # Create collection
        self.client.create_collection(
            collection_name=self.collection_name,
            schema=schema,
            index_params=index_params
        )

    def load_collection(self):
        self.client.load_collection(self.collection_name)

    def insert_documents(self, docs: List[dict]):
        texts = [d.get("text", "") for d in docs]
        text_dense_vecs = self.text_model.encode(texts, convert_to_numpy=True).astype(np.float32).tolist()

        image_dense_vecs = []
        for d in docs:
            b = d.get("image_bytes")
            if b:
                # Compute CLIP image embedding
                img = Image.open(io.BytesIO(b))
                img_input = self.clip_preprocess(img).unsqueeze(0)
                with torch.no_grad():
                    img_features = self.clip_model.encode_image(img_input).cpu().numpy().flatten()
                image_dense_vecs.append(img_features.tolist())
            else:
                image_dense_vecs.append([0.0] * 512)  # Placeholder

        ids = [d.get("id") for d in docs]
        data = [
            {"doc_id": id_, "text": text, "text_dense": dense, "image_dense": img}
            for id_, text, dense, img in zip(ids, texts, text_dense_vecs, image_dense_vecs)
        ]
        self.client.insert(collection_name=self.collection_name, data=data)

    def hybrid_search(self, query: str, image_bytes: bytes = None, topk=10) -> List[Tuple[Any, float, dict]]:
        self.load_collection()
        # ... rest of the method
        query_dense_vector = self.text_model.encode([query], convert_to_numpy=True)[0].astype(np.float32).tolist()

        reqs = []

        # Semantic text search (dense)
        search_param_1 = {
            "data": [query_dense_vector],
            "anns_field": "text_dense",
            "param": {"nprobe": 10},
            "limit": topk
        }
        request_1 = AnnSearchRequest(**search_param_1)
        reqs.append(request_1)

        # Full-text search (sparse)
        search_param_2 = {
            "data": [query],  # For sparse, data is the text query
            "anns_field": "text_sparse",
            "limit": topk
        }
        request_2 = AnnSearchRequest(**search_param_2)
        reqs.append(request_2)

        # Image search if image provided
        if image_bytes:
            img = Image.open(io.BytesIO(image_bytes))
            img_input = self.clip_preprocess(img).unsqueeze(0)
            with torch.no_grad():
                query_image_vector = self.clip_model.encode_image(img_input).cpu().numpy().flatten().tolist()
            search_param_3 = {
                "data": [query_image_vector],
                "anns_field": "image_dense",
                "param": {"nprobe": 10},
                "limit": topk
            }
            request_3 = AnnSearchRequest(**search_param_3)
            reqs.append(request_3)

        # Reranker
        from pymilvus import Function
        ranker = Function(
            name="rrf",
            input_field_names=[],  # Must be empty
            function_type=FunctionType.RERANK,
            params={"reranker": "rrf", "k": 100}
        )

        # Perform hybrid search
        res = self.client.hybrid_search(
            collection_name=self.collection_name,
            reqs=reqs,
            ranker=ranker,
            limit=topk,
            output_fields=["doc_id", "text"]
        )

        out = []
        for hits in res:
            for hit in hits:
                entity = hit.get("entity", {})
                out.append((entity.get("doc_id"), float(hit.get("distance", 0)), "hybrid"))
        return out
